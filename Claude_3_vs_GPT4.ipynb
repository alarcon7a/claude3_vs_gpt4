{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alarcon7a/claude3_vs_gpt4/blob/main/Claude_3_vs_GPT4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BpP_YnWlCKq-"
      },
      "source": [
        "## Instalando algunas librerias necesarias"
      ],
      "id": "BpP_YnWlCKq-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d8859cb5d4d05fa",
        "outputId": "0bf34080-2c8e-48f2-8962-9bf57c952c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.2/850.2 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q -U langchain-anthropic langchain gradio openai"
      ],
      "id": "2d8859cb5d4d05fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "urM6drkqCKrB"
      },
      "source": [
        "## Importando librerias"
      ],
      "id": "urM6drkqCKrB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ba6357d830d135"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "import gradio as gr\n",
        "import time"
      ],
      "id": "a3ba6357d830d135"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb08191142e5242b"
      },
      "outputs": [],
      "source": [
        "from langchain_anthropic import AnthropicLLM\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import HumanMessage, SystemMessage"
      ],
      "id": "fb08191142e5242b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OqE0_hi5CKrC"
      },
      "source": [
        "## Configurando keys"
      ],
      "id": "OqE0_hi5CKrC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab2c0037c1b9d28b",
        "outputId": "bab92f0a-fc9d-4c18-87e0-9684c069399e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ],
      "source": [
        "ANTHROPIC_API_KEY = getpass('Enter the secret value: ')\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY"
      ],
      "id": "ab2c0037c1b9d28b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e592d6ebd4ccb0e5",
        "outputId": "eed332fb-fe5a-4d45-f501-e26bf6a389c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = getpass('Enter the secret value: ')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "id": "e592d6ebd4ccb0e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qKmU_tHmCKrC"
      },
      "source": [
        "# Gemini vs GPT4  - Texto"
      ],
      "id": "qKmU_tHmCKrC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d27cfa5934abb46e"
      },
      "source": [
        "## Claude + Langchain"
      ],
      "id": "d27cfa5934abb46e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56c56c9cc2cb1da7"
      },
      "outputs": [],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "chat_claude = ChatAnthropic(model='claude-3-opus-20240229',temperature=0.8, max_tokens=4096,)\n",
        "\n",
        "template = (\n",
        "    \"You are a helpful assistant\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, human_message_prompt]\n",
        ")\n",
        "\n"
      ],
      "id": "56c56c9cc2cb1da7"
    },
    {
      "cell_type": "code",
      "source": [
        "# get a chat completion from the formatted messages\n",
        "response = chat_claude(\n",
        "    chat_prompt.format_prompt(\n",
        "        text=\"Quien eres?\"\n",
        "    ).to_messages()\n",
        ")"
      ],
      "metadata": {
        "id": "5AZxNTQnqtST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0aaaa6-f92f-4e9c-f29d-e4424439a8d5"
      },
      "id": "5AZxNTQnqtST",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "3fe9ef83e2b62cfa",
        "outputId": "a2dc4da1-68a4-4e6d-c30d-0d54b708ca5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Soy Claude, un asistente virtual de inteligencia artificial creado por Anthropic. Mi propósito es ayudar a los usuarios como tú con una amplia variedad de tareas, desde responder preguntas hasta ayudar con análisis y resolución de problemas. Tengo conocimientos en muchas áreas, pero no soy un ser humano real, sino un programa de computadora diseñado para conversar de manera natural y brindar asistencia útil. Espero poder ayudarte en lo que necesites.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "response.content"
      ],
      "id": "3fe9ef83e2b62cfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3247d1eaf3dea5ef"
      },
      "source": [
        "## OpenAI GPT-4 + Langchain"
      ],
      "id": "3247d1eaf3dea5ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a388828c5f6c395e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38cad47-1b31-42ce-85c4-f93ad74882a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "chat_gpt4 = ChatOpenAI( model=\"gpt-4\",temperature=0.8, max_tokens=4096,)\n",
        "\n",
        "template = (\n",
        "    \"You are a helpful assistant\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, human_message_prompt]\n",
        ")\n"
      ],
      "id": "a388828c5f6c395e"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# get a chat completion from the formatted messages\n",
        "response = chat_gpt4(\n",
        "    chat_prompt.format_prompt(\n",
        "        text=\"Quien eres?\"\n",
        "    ).to_messages()\n",
        ")"
      ],
      "metadata": {
        "id": "HpGaOG4EqvGV"
      },
      "id": "HpGaOG4EqvGV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6d18e1cc1b228f45",
        "outputId": "bddf882c-b6f6-4b23-d148-3ca32f96bf18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Soy un asistente inteligente diseñado para proporcionar ayuda e información. ¿Cómo puedo ayudarte hoy?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "response.content"
      ],
      "id": "6d18e1cc1b228f45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cfd3915ed553c54c"
      },
      "source": [
        "## Funciones"
      ],
      "id": "cfd3915ed553c54c"
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_response(text, claude=True):\n",
        "    if claude:\n",
        "        model = chat_claude\n",
        "    else:\n",
        "        model = chat_gpt4\n",
        "    start_time = time.time()  # Inicia el contador de tiempo\n",
        "    response = model(\n",
        "        chat_prompt.format_prompt(\n",
        "            text=text\n",
        "        ).to_messages()\n",
        "    ) # Invoca el modelo\n",
        "    end_time = time.time()  # Finaliza el contador de tiempo\n",
        "    execution_time = end_time - start_time\n",
        "    return response.content, execution_time"
      ],
      "metadata": {
        "id": "tCnex9K_-sSo"
      },
      "id": "tCnex9K_-sSo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f25786f6090c7945"
      },
      "outputs": [],
      "source": [
        "def claude_vs_gpt4(text_question):\n",
        "    claude_list = []\n",
        "    gpt_list = []\n",
        "    claude_content, claude_execution_time = chat_response(text_question)\n",
        "    gpt4_content, gpt4_execution_time = chat_response(text_question, claude=False)\n",
        "\n",
        "    claude_time_formatted = f\"{claude_execution_time:.2f} segundos\"\n",
        "    gpt4_time_formatted = f\"{gpt4_execution_time:.2f} segundos\"\n",
        "\n",
        "    claude_list.append((text_question, claude_content))\n",
        "    gpt_list.append((text_question, gpt4_content))\n",
        "    return claude_list, claude_time_formatted, gpt_list, gpt4_time_formatted"
      ],
      "id": "f25786f6090c7945"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-yhMuc5gCKrE"
      },
      "source": [
        "## Gradio"
      ],
      "id": "-yhMuc5gCKrE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "805de4e7d1b35ed1",
        "outputId": "bbef96e1-b189-4402-b831-ac445442a290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://cc15a417268d5b5374.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cc15a417268d5b5374.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cc15a417268d5b5374.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            txt_time_claude = gr.Textbox(label=\"Tiempo de ejecución de Claude 3\", interactive=False)\n",
        "            chat_response_claude = gr.Chatbot(label=\"Respuesta de Claude 3\")\n",
        "        with gr.Column():\n",
        "            txt_time_gpt = gr.Textbox(label=\"Tiempo de ejecución de GPT-4\", interactive=False)\n",
        "            chat_response_gpt = gr.Chatbot(label=\"Respuesta de GPT-4\",)\n",
        "    with gr.Row():\n",
        "        txt_question = gr.Textbox(label=\"Pregunta...\")\n",
        "        btn_send = gr.Button(value=\"Enviar\")\n",
        "        btn_send.click(\n",
        "            claude_vs_gpt4,\n",
        "            inputs=txt_question,\n",
        "            outputs=[chat_response_claude, txt_time_claude, chat_response_gpt, txt_time_gpt]\n",
        "        )\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "id": "805de4e7d1b35ed1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2eefccd323641a6"
      },
      "outputs": [],
      "source": [
        "demo.close()"
      ],
      "id": "c2eefccd323641a6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}